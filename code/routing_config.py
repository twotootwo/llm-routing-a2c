
# routing_config.py

from dataclasses import dataclass
from typing import Literal, List, Tuple
import os

import torch

from squad_dataset import SQuADDataset
from llm_wrapper import HFLLMWrapper
from qa_span_wrapper import SquadQAModelWrapper
from qa_env import QARoutingEnv

# qa: span extractor (SquadQAModelWrapper)
# lm: generate (HFLLMWrapper)
ModelType = Literal["qa", "lm"]

# ğŸ”§ ì—¬ê¸° ì´ë¦„/íƒ€ì…ë§Œ ë°”ê¿”ê°€ë©´ì„œ ì‹¤í—˜í•˜ë©´ ë¨
#CHEAP_MODEL_NAME = "deepset/tinyroberta-squad2"
CHEAP_MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
CHEAP_MODEL_TYPE: ModelType = "lm"

STRONG_MODEL_NAME = "deepset/roberta-large-squad2"
STRONG_MODEL_TYPE: ModelType = "qa"

# ì‹¤í—˜ í´ë” ì´ë¦„: qa+qa, lm+qa, lm+lm ë“± ìë™ êµ¬ì„±
DIR = f"{CHEAP_MODEL_TYPE}+{STRONG_MODEL_TYPE}"
CHECKPOINT_DIR = f"{DIR}/checkpoint"
RESULT_DIR = f"{DIR}/results"


# ---- ê°œë³„ ëª¨ë¸ ì„¤ì • ----
@dataclass
class ModelRoutingConfig:
    model_name: str          # HF model name
    model_type: ModelType    # "qa" or "lm"
    device: str | None = None
    max_new_tokens: int = 64     # lmì¼ ë•Œë§Œ ì‚¬ìš©
    temperature: float = 0.0     # lmì¼ ë•Œë§Œ ì‚¬ìš©


# ---- ì „ì²´ ë¼ìš°íŒ… + ë¡œê¹…/ì²´í¬í¬ì¸íŠ¸ ì„¤ì • ----
@dataclass
class RoutingConfig:
    cheap: ModelRoutingConfig
    strong: ModelRoutingConfig

    # Env reward ê´€ë ¨
    max_retry: int = 2
    token_budget: int = 512
    w_token: float = 0.0    # QA-onlyë©´ 0, LM ì“¸ ë•Œ >0ë¡œ ì„¤ì •
    w_retry: float = 0.2
    w_strong: float = 0.7

    # ì²´í¬í¬ì¸íŠ¸ / ê²°ê³¼ ì €ì¥ ê´€ë ¨
    checkpoint_dir: str = "trained_model"
    result_dir: str = "results"
    checkpoint_episodes: List[int] | None = None


def default_routing_config() -> RoutingConfig:
    """
    ğŸ‘‰ ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ë©´ ë¨.
    cheap / strong / reward weight / í´ë” ì´ë¦„ê¹Œì§€ ì „ë¶€ í•œ êµ°ë°ì—ì„œ.
    """
    default_device = "cuda" if torch.cuda.is_available() else "cpu"

    # ===== cheap ëª¨ë¸ ì„¤ì • =====
    if CHEAP_MODEL_TYPE == "qa":
        cheap_cfg = ModelRoutingConfig(
            model_name=CHEAP_MODEL_NAME,
            model_type=CHEAP_MODEL_TYPE,
            device=default_device,
        )
    else:  # "lm"
        cheap_cfg = ModelRoutingConfig(
            model_name=CHEAP_MODEL_NAME,
            model_type=CHEAP_MODEL_TYPE,
            device=default_device,
            max_new_tokens=64,
            temperature=0.0,
        )

    # ===== strong ëª¨ë¸ ì„¤ì • =====
    if STRONG_MODEL_TYPE == "qa":
        strong_cfg = ModelRoutingConfig(
            model_name=STRONG_MODEL_NAME,
            model_type=STRONG_MODEL_TYPE,
            device=default_device,
        )
    else:  # "lm"
        strong_cfg = ModelRoutingConfig(
            model_name=STRONG_MODEL_NAME,
            model_type=STRONG_MODEL_TYPE,
            device=default_device,
            max_new_tokens=64,
            temperature=0.0,
        )

    # ===== env + ë¡œê¹…/ì²´í¬í¬ì¸íŠ¸ ì„¤ì • =====
    return RoutingConfig(
        cheap=cheap_cfg,
        strong=strong_cfg,
        max_retry=2,
        token_budget=512,
        w_token=0.01,            # QA-onlyë©´ 0
        w_retry=0.2,
        w_strong=0.7,
        checkpoint_dir=CHECKPOINT_DIR,
        result_dir=RESULT_DIR,  # ê²°ê³¼ ì €ì¥ í´ë” ì´ë¦„
        checkpoint_episodes=None,
    )


# ---- ë‚´ë¶€: ëª¨ë¸ config -> ì‹¤ì œ wrapper ìƒì„± ----
def _build_model_from_cfg(cfg: ModelRoutingConfig):
    """
    cfg.model_typeì— ë”°ë¼ HFLLMWrapper ë˜ëŠ” SquadQAModelWrapper ìƒì„±.
    ë°˜í™˜: (wrapper ê°ì²´, kind ë¬¸ìì—´, ì‹¤ì œ device)
    """
    device = cfg.device or ("cuda" if torch.cuda.is_available() else "cpu")

    if cfg.model_type == "lm":
        model = HFLLMWrapper(
            model_name=cfg.model_name,
            device=device,
            max_new_tokens=cfg.max_new_tokens,
            temperature=cfg.temperature,
        )
    elif cfg.model_type == "qa":
        model = SquadQAModelWrapper(
            model_name=cfg.model_name,
            device=device,
        )
    else:
        raise ValueError(f"Unknown model_type: {cfg.model_type} (expected 'qa' or 'lm')")

    return model, cfg.model_type, device


# ---- ì™¸ë¶€ì—ì„œ ì“°ëŠ” ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ----
def build_env_and_paths(
    split: str = "train",
    config: RoutingConfig | None = None,
) -> Tuple[QARoutingEnv, str, str, List[int] | None]:
    """
    - âœ… í•­ìƒ SQuAD(= QA dataset)ë§Œ ì‚¬ìš©
    - cheap / strong wrapper ìƒì„± (qa ë˜ëŠ” lm)
    - QARoutingEnv ìƒì„±
    - checkpoint_dir / result_dir / checkpoint_episodes í•¨ê»˜ ë¦¬í„´
    """
    if config is None:
        config = default_routing_config()

    # 1) Dataset: âœ… LM/QA ìƒê´€ì—†ì´ ë¬´ì¡°ê±´ SQuADë§Œ ì‚¬ìš©
    dataset = SQuADDataset(split=split)
    print(f"[Dataset] QA (SQuAD) for split={split}: {len(dataset)} samples")

    # 2) cheap/strong ëª¨ë¸ ìƒì„±
    cheap_model, cheap_kind, cheap_device = _build_model_from_cfg(config.cheap)
    strong_model, strong_kind, strong_device = _build_model_from_cfg(config.strong)

    print(f"Cheap model  : {config.cheap.model_name} (type={cheap_kind}, device={cheap_device})")
    print(f"Strong model : {config.strong.model_name} (type={strong_kind}, device={strong_device})")

    # 3) Env ìƒì„±
    env = QARoutingEnv(
        dataset=dataset,
        cheap_model=cheap_model,
        cheap_kind=cheap_kind,         # "qa" or "lm"
        strong_model=strong_model,
        strong_kind=strong_kind,       # "qa" or "lm"
        max_retry=config.max_retry,
        token_budget=config.token_budget,
        w_token=config.w_token,
        w_retry=config.w_retry,
        w_strong=config.w_strong,
    )

    # 4) í´ë” ìƒì„±
    os.makedirs(DIR,exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)
    os.makedirs(config.result_dir, exist_ok=True)

    return env, config.checkpoint_dir, config.result_dir, config.checkpoint_episodes

