import os
import glob
import numpy as np
import torch
import random   # ðŸ‘ˆ ì¶”ê°€

from qa_env import QARoutingEnv, ACTION_ACCEPT, ACTION_ESCALATE, ACTION_RETRY
from train_a2c import ActorCritic
from routing_config import build_env_and_paths


# ---------- 1. ì—í”¼ì†Œë“œ í•œ ë²ˆ ì‹¤í–‰í•˜ëŠ” ê³µí†µ í•¨ìˆ˜ ----------

def run_episode(env: QARoutingEnv, policy_fn, model=None, device="cpu"):
    """
    env + policy_fn(ë° optional A2C model)ì„ ì‚¬ìš©í•´
    ì—í”¼ì†Œë“œ 1ê°œë¥¼ ëŒë¦¬ê³ ,
    ì´ reward / ìµœì¢… F1 / ì´ í† í° ìˆ˜ / action ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.
    """
    state = env.reset()
    done = False
    total_reward = 0.0
    final_f1 = 0.0
    total_tokens = 0
    actions = []   # ðŸ‘ˆ ì—í”¼ì†Œë“œ ë™ì•ˆì˜ action ê¸°ë¡

    while not done:
        action = policy_fn(state, env, model, device)
        next_state, reward, done, info = env.step(action)

        total_reward += reward
        state = next_state
        actions.append(int(action))

        if "answer_score" in info:
            final_f1 = info["answer_score"]
        if "total_tokens" in info:
            total_tokens = info["total_tokens"]

    return total_reward, final_f1, total_tokens, actions


# ---------- 2. ì—¬ëŸ¬ ì—í”¼ì†Œë“œì— ëŒ€í•´ í‰ê·  ë‚´ê¸° ----------

def evaluate_policy(env, policy_fn, model=None, device="cpu", num_episodes=100):
    rewards = []
    f1s = []
    tokens = []
    all_actions = []   # ðŸ‘ˆ ì „ì²´ ì—í”¼ì†Œë“œ action ëª¨ìœ¼ê¸°

    base_seed = 1234
    np.random.seed(base_seed)
    random.seed(base_seed)
    torch.manual_seed(base_seed)

    for _ in range(num_episodes):
        r, f1, tok, actions = run_episode(env, policy_fn, model, device)
        rewards.append(r)
        f1s.append(f1)
        tokens.append(tok)
        all_actions.extend(actions)

    # ðŸ‘‡ action ë¹„ìœ¨ ê³„ì‚°
    p_accept = p_retry = p_escalate = 0.0
    if len(all_actions) > 0:
        total_actions = len(all_actions)
        num_accept = all_actions.count(ACTION_ACCEPT)
        num_retry = all_actions.count(ACTION_RETRY)
        num_escalate = all_actions.count(ACTION_ESCALATE)

        p_accept = num_accept / total_actions
        p_retry = num_retry / total_actions
        p_escalate = num_escalate / total_actions

    return {
        "avg_reward": float(np.mean(rewards)),
        "avg_f1": float(np.mean(f1s)),
        "avg_tokens": float(np.mean(tokens)),
        "p_accept": float(p_accept),
        "p_retry": float(p_retry),
        "p_escalate": float(p_escalate),
    }


# ---------- 3. Baseline policy ì •ì˜ë“¤ ----------

def policy_always_cheap(state, env, model, device):
    return ACTION_ACCEPT


def policy_always_escalate(state, env, model, device):
    return ACTION_ESCALATE


def policy_random(state, env, model, device):
    return int(np.random.choice([ACTION_ACCEPT, ACTION_ESCALATE]))


def policy_trained_a2c(state, env, model: ActorCritic, device):
    state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)
    with torch.no_grad():
        logits, _ = model(state_tensor)
    action = torch.argmax(logits, dim=-1).item()
    return int(action)


# ---------- 4. main ----------

def main():
    # policy networkìš© ë””ë°”ì´ìŠ¤ (env ë‚´ë¶€ cheap/strongì€ routing_configì—ì„œ ì´ë¯¸ ì •í•´ì§)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Eval device (policy net): {device}")

    # âœ… routing_config ê¸°ë°˜ìœ¼ë¡œ env + ê²½ë¡œë“¤ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
    # split="validation" ê³ ì • evaluation
    env, checkpoint_dir, result_dir, _ = build_env_and_paths(split="validation")

    num_eval_episodes = 100

    # ----- Baselines í‰ê°€ -----
    print("\n=== Evaluating baselines ===")
    res_cheap = evaluate_policy(env, policy_always_cheap, None, device, num_eval_episodes)
    print(
        f"Always Cheap    -> reward={res_cheap['avg_reward']:.4f}, "
        f"F1={res_cheap['avg_f1']:.4f}, tokens={res_cheap['avg_tokens']:.1f}, "
        f"action_dist(A/R/E)={res_cheap['p_accept']:.2f}/"
        f"{res_cheap['p_retry']:.2f}/{res_cheap['p_escalate']:.2f}"
    )

    res_escalate = evaluate_policy(env, policy_always_escalate, None, device, num_eval_episodes)
    print(
        f"Always Escalate -> reward={res_escalate['avg_reward']:.4f}, "
        f"F1={res_escalate['avg_f1']:.4f}, tokens={res_escalate['avg_tokens']:.1f}, "
        f"action_dist(A/R/E)={res_escalate['p_accept']:.2f}/"
        f"{res_escalate['p_retry']:.2f}/{res_escalate['p_escalate']:.2f}"
    )

    res_random = evaluate_policy(env, policy_random, None, device, num_eval_episodes)
    print(
        f"Random          -> reward={res_random['avg_reward']:.4f}, "
        f"F1={res_random['avg_f1']:.4f}, tokens={res_random['avg_tokens']:.1f}, "
        f"action_dist(A/R/E)={res_random['p_accept']:.2f}/"
        f"{res_random['p_retry']:.2f}/{res_random['p_escalate']:.2f}"
    )

    # ----- A2C ì²´í¬í¬ì¸íŠ¸ë“¤ í‰ê°€ -----
    print("\n=== Evaluating trained A2C checkpoints ===")
    state_dim = 11   # í˜„ìž¬ state_features ê¸¸ì´
    action_dim = 3   # [ACCEPT, RETRY, ESCALATE]

    ckpt_paths = glob.glob(os.path.join(checkpoint_dir, "a2c_actor_critic_ep*.pt"))

    def extract_ep_num(path: str) -> int:
        base = os.path.basename(path)
        num_part = base.replace("a2c_actor_critic_ep", "").replace(".pt", "")
        try:
            return int(num_part)
        except ValueError:
            return 0

    ckpt_paths = sorted(ckpt_paths, key=extract_ep_num)

    if not ckpt_paths:
        print("No checkpoint files found under", checkpoint_dir)
    else:
        for ckpt_path in ckpt_paths:
            ep_num = extract_ep_num(ckpt_path)
            model = ActorCritic(state_dim, action_dim).to(device)
            state_dict = torch.load(ckpt_path, map_location=device)
            try:
                model.load_state_dict(state_dict)
            except RuntimeError as e:
                print(f"âš ï¸ Skipping checkpoint {ckpt_path} (shape mismatch: {e})")
                continue

            model.eval()
            res_a2c = evaluate_policy(env, policy_trained_a2c, model, device, num_eval_episodes)
            print(
                f"A2C (ep={ep_num:5d}) -> reward={res_a2c['avg_reward']:.4f}, "
                f"F1={res_a2c['avg_f1']:.4f}, tokens={res_a2c['avg_tokens']:.1f}, "
                f"action_dist(A/R/E)={res_a2c['p_accept']:.2f}/"
                f"{res_a2c['p_retry']:.2f}/{res_a2c['p_escalate']:.2f} "
                f"[ckpt: {ckpt_path}]"
            )

    # ----- ìµœì¢… ëª¨ë¸ í‰ê°€ (RESULT_DIR ì•„ëž˜ì— ì €ìž¥ëœ ê²ƒ) -----
    final_ckpt = os.path.join(result_dir, "a2c_actor_critic_squad_final.pt")
    if os.path.exists(final_ckpt):
        print("\n=== Evaluating FINAL A2C model ===")
        model = ActorCritic(state_dim, action_dim).to(device)
        state_dict = torch.load(final_ckpt, map_location=device)
        try:
            model.load_state_dict(state_dict)
        except RuntimeError as e:
            print(f"âš ï¸ FINAL checkpoint shape mismatch, skipping: {e}")
        else:
            model.eval()
            res_final = evaluate_policy(env, policy_trained_a2c, model, device, num_eval_episodes)
            print(
                f"A2C (FINAL)    -> reward={res_final['avg_reward']:.4f}, "
                f"F1={res_final['avg_f1']:.4f}, tokens={res_final['avg_tokens']:.1f}, "
                f"action_dist(A/R/E)={res_final['p_accept']:.2f}/"
                f"{res_final['p_retry']:.2f}/{res_final['p_escalate']:.2f} "
                f"[ckpt: {final_ckpt}]"
            )
    else:
        print("\nNo final A2C checkpoint found:", final_ckpt)


if __name__ == "__main__":
    main()
